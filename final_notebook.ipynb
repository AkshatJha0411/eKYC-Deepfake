{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note : This notebook was used in Google Colab thus paths will have to be changed accordingly (if running elsewhere or in a different path)\n",
        "\n",
        "#### Link to colab notebook: https://colab.research.google.com/drive/1dUSQPJc7kcRZZE7oIdfXMFR9HE4xKGdT?usp=sharing\n",
        "\n",
        "#### Link to Dataset : https://drive.google.com/file/d/1LnvVO5eJiSwjOeu7SmNVDDkD9qp6BB9e/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-c7vPNskcVU"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch albumentations torch torchvision facenet-pytorch albumentations opencv-python pandas numpy scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXE2Qa7-kd1-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from albumentations import Compose, RandomBrightnessContrast, HueSaturationValue\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import functional as F\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XayIRblOkgOv"
      },
      "outputs": [],
      "source": [
        "# Ensure GPU is being used\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU detected:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Path to your dataset\n",
        "BASE_PATH = '/content/drive/MyDrive/Hackathon/ZenTej/Sentinel_FaceV1'\n",
        "# (or Kaggle use '/kaggle/input/sentinel-facev1/Sentinel_FaceV1' accordingly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX06OLBdkkMj"
      },
      "outputs": [],
      "source": [
        "labels_df = pd.read_csv(os.path.join(BASE_PATH, 'Forgery_Dataset', 'train_labels.csv'))\n",
        "\n",
        "# Filter valid entries only\n",
        "valid_df = labels_df[labels_df.apply(lambda row: os.path.exists(os.path.join(BASE_PATH, 'Forgery_Dataset', row['image_path'])), axis=1)]\n",
        "print(f\"Using {len(valid_df)} valid entries out of {len(labels_df)}\")\n",
        "\n",
        "label_map = {'real': 0, 'fake': 1}\n",
        "\n",
        "# Dataset class\n",
        "class ForgeryDataset(Dataset):\n",
        "    def __init__(self, df, base_path, transform=None):\n",
        "        self.df = df\n",
        "        self.base_path = base_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            img_path = os.path.join(self.base_path, 'Forgery_Dataset', row['image_path'])\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                raise FileNotFoundError(f\"Image not found or corrupt: {img_path}\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            label = label_map[row['label']]\n",
        "\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=image)\n",
        "                image = augmented['image']\n",
        "\n",
        "            image = image.transpose(2, 0, 1)\n",
        "            image = torch.from_numpy(image).float() / 255.0\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading idx {idx}: {e} - Skipping\")\n",
        "            return None, None\n",
        "\n",
        "# Augmentation transforms\n",
        "aug_transform = Compose([\n",
        "    RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "])\n",
        "\n",
        "# Collate to filter skipped data\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item[0] is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None, None\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpnWpTDeklRZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = ForgeryDataset(valid_df, BASE_PATH, transform=aug_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "deepfake_model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1).to(device)\n",
        "in_features = deepfake_model.classifier[1].in_features\n",
        "deepfake_model.classifier[1] = nn.Linear(in_features, 2).to(device)\n",
        "\n",
        "optimizer = Adam(deepfake_model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-vxgupLknqk"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    deepfake_model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_dataloader:\n",
        "        if images is None:\n",
        "            continue\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        dct_imgs = torch.fft.fft2(images).real\n",
        "        outputs = deepfake_model(dct_imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_dataloader)}\")\n",
        "\n",
        "torch.save(deepfake_model.state_dict(), '/content/deepfake_model.pth')\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfKoXzvilLOY"
      },
      "outputs": [],
      "source": [
        "def process_input(input1, input2=None, is_video=False):\n",
        "    if is_video:\n",
        "        cap = cv2.VideoCapture(input1)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            face = mtcnn(frame)\n",
        "            if face is not None:\n",
        "                frames.append(face)\n",
        "        cap.release()\n",
        "        if not frames:\n",
        "            return None, None, None\n",
        "        face1 = frames[0]\n",
        "        liveness_score = np.var([f.mean().item() for f in frames]) > 0.1\n",
        "    else:\n",
        "        face1 = mtcnn(input1)\n",
        "        liveness_score = 1.0\n",
        "\n",
        "    if input2:\n",
        "        face2 = mtcnn(input2)\n",
        "        if face1 is None or face2 is None:\n",
        "            return None, None, None\n",
        "        emb1 = resnet(face1.unsqueeze(0).to(device)).detach().cpu().numpy()\n",
        "        emb2 = resnet(face2.unsqueeze(0).to(device)).detach().cpu().numpy()\n",
        "        match_score = cosine_similarity(emb1, emb2)[0][0]\n",
        "    else:\n",
        "        match_score = None\n",
        "\n",
        "    if face1 is not None:\n",
        "        face1 = face1.to(device)\n",
        "        dct1 = torch.fft.fft2(face1.unsqueeze(0)).real\n",
        "        auth_pred = deepfake_model(dct1)\n",
        "        authenticity_prob = F.softmax(auth_pred, dim=1)[0][0].item()\n",
        "        authenticity = 'Authentic' if authenticity_prob > 0.5 else 'Forged'\n",
        "    else:\n",
        "        authenticity = None\n",
        "\n",
        "    return match_score, liveness_score, authenticity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd-tBOIPlMPC"
      },
      "outputs": [],
      "source": [
        "def grad_cam(model, input_tensor, target_class=0):\n",
        "    model.eval()\n",
        "    grads = []\n",
        "    activations = []\n",
        "\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        grads.append(grad_out[0])\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output)\n",
        "\n",
        "    b_handle = model.features[-1].register_backward_hook(backward_hook)\n",
        "    f_handle = model.features[-1].register_forward_hook(forward_hook)\n",
        "\n",
        "    output = model(input_tensor)\n",
        "    model.zero_grad()\n",
        "    target_class = min(target_class, output.size(1) - 1)\n",
        "    output[0][target_class].backward()\n",
        "\n",
        "    pooled_grads = torch.mean(grads[0], dim=[0, 2, 3])\n",
        "    heatmap = torch.sum(pooled_grads.unsqueeze(-1).unsqueeze(-1) * activations[0], dim=1)\n",
        "    heatmap = F.relu(heatmap)\n",
        "\n",
        "    b_handle.remove()\n",
        "    f_handle.remove()\n",
        "\n",
        "    return heatmap.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvDPpOWVlO2I"
      },
      "outputs": [],
      "source": [
        "sample_img_path = os.path.join(BASE_PATH, 'Forgery_Dataset/real/10564.jpg')\n",
        "sample_img = cv2.imread(sample_img_path)\n",
        "sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
        "sample_face_tensor = mtcnn(sample_img)\n",
        "\n",
        "if sample_face_tensor is not None and len(sample_face_tensor) > 0:\n",
        "    sample_face = sample_face_tensor[0].unsqueeze(0).to(device)\n",
        "    sample_dct = torch.fft.fft2(sample_face).real\n",
        "    heatmap = grad_cam(deepfake_model, sample_dct, target_class=0)\n",
        "    plt.imshow(heatmap[0], cmap='hot')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No face detected in the sample image for Grad-CAM.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrWYsFbElQlH"
      },
      "outputs": [],
      "source": [
        "torch.save(deepfake_model.state_dict(), '/content/deepfake_model.pth')\n",
        "print(\"Model saved at /content/deepfake_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
